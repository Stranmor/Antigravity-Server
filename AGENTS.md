# Antigravity Manager - Architecture Status

## TARGET GOAL
- Remove request/response content capture from proxy monitoring while keeping metadata-only logging and UI handling.

## Current Status
- In progress: content capture removal and UI fallback adjustments implemented; awaiting final verification entry.

## âœ… COMPLETED: PostgreSQL Migration [2026-02-03]

**Goal:** Replace JSON file storage with PostgreSQL + Event Sourcing â€” **DEPLOYED**

### Verification Results

| Metric | Value |
|--------|-------|
| Accounts migrated | 41 |
| Tokens migrated | 41 |
| API `/api/accounts` | âœ… Working |
| API `/api/status` | âœ… Working |
| Chat completions | âœ… Working |
| Database | PostgreSQL 16 on VPS |

### Migration Phases

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Add sqlx + PostgreSQL deps | âœ… |
| 2 | Create migration files (schema) | âœ… |
| 3 | Implement `AccountRepository` trait | âœ… |
| 4 | PostgreSQL backend implementation | âœ… |
| 5 | Event sourcing: `AccountEvent` enum | âœ… |
| 6 | JSON â†’ PostgreSQL data migration | âœ… |
| 7 | Wire repository into AppState + main.rs | âœ… |
| 8 | Setup PostgreSQL on VPS (NixOS) | âœ… |
| 9 | Update API handlers to use repository | âœ… |
| 10 | Deploy + verify | âœ… |

### Database Configuration

| Setting | Value |
|---------|-------|
| Host | 127.0.0.1 |
| Database | antigravity |
| User | antigravity |
| Tables | accounts, tokens, quotas, account_events, requests, app_settings, thinking_signatures, session_signatures |

### Database Replication [2026-02-08]

VPS PostgreSQL Ñ€ĞµĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° home-server Ñ‡ĞµÑ€ĞµĞ· streaming replication. Ğ¡Ğ¼. Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AGENTS.md ÑĞµĞºÑ†Ğ¸Ñ 3.5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹.

| Ğ Ğ¾Ğ»ÑŒ | Ğ“Ğ´Ğµ | Connection string |
|------|-----|-------------------|
| **Primary (read-write)** | VPS | `postgres://antigravity@localhost/antigravity?host=/run/postgresql` |
| **Replica (read-only)** | home-server:5436 | `postgres://antigravity@192.168.0.124:5436/antigravity` |

- VPS: `wal_level=replica`, `max_wal_senders=10`, `wal_keep_size=1GB`
- Replication user: `replicator` (password Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ AGENTS.md)
- SSH tunnel: `pg-replication-tunnel.service` Ğ½Ğ° home-server

### Files Modified

- `Cargo.toml` (workspace) â€” added sqlx, async-trait
- `crates/antigravity-core/Cargo.toml` â€” added sqlx, async-trait
- `crates/antigravity-core/migrations/001_initial_schema.sql` â€” PostgreSQL schema (with IF NOT EXISTS)
- `crates/antigravity-core/src/modules/repository.rs` â€” AccountRepository trait
- `crates/antigravity-core/src/modules/account_pg.rs` â€” PostgreSQL implementation
- `crates/antigravity-core/src/modules/json_migration.rs` â€” Migration utilities
- `antigravity-server/Cargo.toml` â€” added sqlx
- `antigravity-server/src/main.rs` â€” DATABASE_URL parsing, PostgresAccountRepository init
- `antigravity-server/src/state.rs` â€” Added repository to AppState
- `antigravity-server/src/api/mod.rs` â€” Updated handlers to use repository when available
- NixOS config `/etc/nixos/configuration.nix` â€” Added DATABASE_URL to antigravity.service

---

## ğŸ›ï¸ ARCHITECTURAL EVOLUTION [2026-02-02]

**Current Status:** PHASE 5 COMPLETE â€” Module size compliance refactoring

### âœ… Completed Phases (1-4)

- **Phase 1:** `antigravity-types` crate, Typed Errors, Protocol types, Resilience API, Prometheus Metrics
- **Phase 2:** Replace symlinks with local copies, Remove `#[path]` includes
- **Phase 3:** Validator integration, Re-exports cleanup, Clippy compliance (all 23 modules clean)
- **Phase 4:** Eliminate `antigravity-shared`, Edition 2021 alignment

### ğŸ”„ Phase 5: Module Size Compliance [COMPLETE - 2026-02-04]

**Goal:** Split all files exceeding 300 lines to comply with Single Responsibility Module principle.

**Status:** âœ… ALL `.rs` files now under 300 lines (except test files which are exempt).

**Completed refactoring:**
- `mappers/claude/request.rs` â†’ `mappers/claude/request/` directory (13 modules) âœ…
- `handlers/claude.rs` â†’ `handlers/claude/` directory (5 modules) âœ…
- `handlers/openai.rs` â†’ `handlers/openai/` directory âœ…
- `token_manager/mod.rs` â†’ 13 modules âœ…
- `mappers/claude/streaming.rs` â†’ `mappers/claude/streaming/` directory (7 modules) âœ…
- `mappers/openai/streaming.rs` â†’ `mappers/openai/streaming/` directory (6 modules) âœ…
- `mappers/gemini/wrapper.rs` â†’ extracted tests to `wrapper_tests.rs` âœ…
- `modules/device.rs` â†’ extracted tests to `device_tests.rs` âœ…
- `antigravity-server/main.rs` â†’ extracted `server_utils.rs` + `router.rs` âœ…
- `src-leptos/pages/settings.rs` (549) â†’ `settings/` directory (7 modules, max 134 lines) âœ…
- `src-leptos/pages/dashboard.rs` (399) â†’ `dashboard/` directory (4 modules, max 217 lines) âœ…
- `src-leptos/components/add_account_modal.rs` (379) â†’ `add_account_modal/` directory (3 modules) âœ…
- `modules/migration.rs` (306) â†’ extracted `token_extraction.rs` (265 lines) âœ…

**Exempt (test files):**
- `request_tests.rs` (614 lines)
- `handlers.rs` (378 lines)

### ğŸ“Š Architecture (Current)

```
crates/
â”œâ”€â”€ antigravity-types/          # ğŸ”µ SINGLE SOURCE OF TRUTH (canonical definitions)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ error/              # AccountError, ProxyError, ConfigError, TypedError
â”‚       â”œâ”€â”€ models/             # Account, AppConfig, ProxyConfig, QuotaData, TokenData...
â”‚       â”‚   â”œâ”€â”€ account.rs      # (pub mod)
â”‚       â”‚   â”œâ”€â”€ config.rs       # (pub mod)
â”‚       â”‚   â”œâ”€â”€ quota.rs        # (pub mod)
â”‚       â”‚   â”œâ”€â”€ stats.rs        # (pub mod)
â”‚       â”‚   â”œâ”€â”€ sync.rs         # (pub mod)
â”‚       â”‚   â””â”€â”€ token.rs        # (pub mod)
â”‚       â””â”€â”€ protocol/           # OpenAI/Claude/Gemini message types
â”œâ”€â”€ antigravity-client/         # ğŸŸ£ RUST SDK (auto-discovery, retry, streaming)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ client.rs           # AntigravityClient with auto_discover()
â”‚       â”œâ”€â”€ error.rs            # ClientError enum
â”‚       â””â”€â”€ messages.rs         # ChatRequest, ChatResponse, StreamChunk (SDK-specific)
â”œâ”€â”€ antigravity-core/           # ğŸŸ¢ BUSINESS LOGIC (all clippy-clean!)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ modules/            # Account storage, repository, JSON migration
â”‚       â””â”€â”€ proxy/
â”‚           â””â”€â”€ 23 modules      # ALL modules now clippy-clean
â”œâ”€â”€ antigravity-server/         # ğŸ”´ HTTP ENTRY POINT
vendor/
â””â”€â”€ antigravity-upstream/       # Git submodule (REFERENCE ONLY)
```

> **Note:** `antigravity-shared` has been ELIMINATED (2026-01-28). All code now imports directly from `antigravity-types`.

### ğŸ¯ Key Metrics

| Metric | Before | After |
|--------|--------|-------|
| Symlinks | 14 | **0** |
| Duplicate type definitions | ~20 | **0** |
| `#[allow(warnings)]` | 11 modules | **0** |
| Clippy warnings suppressed | ~58 | **0** |
| Unit tests | - | **346** |
| Integration tests | - | **1** |
| Clippy status | âš ï¸ | **âœ… -D warnings** |
| Release build | - | **11MB** |

### â­ï¸ Remaining Tasks

- [x] **VPS deployment** âœ… [2026-01-19] â€” `https://antigravity.quantumind.ru`
- [x] **Phase 5:** Module Size Compliance âœ… [2026-02-04]
- [x] **CLI Management** â€” full headless control without Web UI âœ… [2026-01-19]
- [x] **Rust SDK** (`antigravity-client`) â€” auto-discovery, retry, streaming âœ… [2026-01-19]
- [x] **Account auto-sync** (60s interval) âœ… [2026-01-19]
- [ ] **Extract `antigravity-proxy` crate** (optional cleanup)

### âš ï¸ Known Issues (Tech Debt)

| File | Issue | Severity |
|------|-------|----------|
| `proxy/common/json_schema/recursive.rs` | `if/else if/else` for `properties`/`items` is mutually exclusive â€” if schema has BOTH, `items` won't be recursively cleaned | Medium |
| `proxy/common/json_schema/recursive.rs` | `else` fallback block treats ALL remaining fields as schemas â€” data fields like `enum` values or `const` containing object-like structures could be corrupted by normalization | Low |
| `proxy/token_manager/token_refresh.rs` | Missing refresh_token update: if OAuth provider returns a new refresh_token (rotation), only access_token is updated in memory â€” old refresh_token persisted | Medium |
| `proxy/token_manager/persistence.rs` | `file_locks` DashMap grows unbounded â€” one Mutex per account_id, never cleaned up | Low |
| `proxy/token_manager/store.rs` | JSON indexing (`content["token"]["field"]`) can panic if file has unexpected structure (not object, missing "token" key) | Low |
| `proxy/token_manager/persistence.rs` | JSON indexing in `save_project_id_to_file`/`save_refreshed_token_to_file` can panic if "token" key missing from file | Low |
| `proxy/token_manager/selection_helpers.rs` | Hardcoded fallback project_id `"bamboo-precept-lgxtn"` when fetch_project_id fails â€” should propagate error | Medium |
| `modules/account_pg_events.rs` | `update_quota_impl` atomicity: quota update and audit log are separate operations on `pool`, not wrapped in transaction â€” audit trail can be lost if log INSERT fails | Low |
| `modules/account_pg_events.rs` | UUID parse errors mapped to `RepositoryError::NotFound` instead of validation error â€” conflates bad input with missing resource | Low |
| `modules/repository.rs` | `update_token_credentials` accepts both `expires_in` and `expiry_timestamp` â€” redundant, allows conflicting data | Low |
| `proxy/token_manager/mod.rs` | `active_requests` DashMap entries never cleaned up when count drops to zero â€” grows unbounded with unique emails | Low |
| `server_utils.rs` | Hardcoded `Domain::IPV4` prevents IPv6 binding; `format!("{}:{}")` generates invalid syntax for IPv6 addresses â€” should parse `IpAddr` first and derive domain | Low |
| `proxy/handlers/gemini/models.rs` | `handle_get_model` accepts any model_name without validation against available models, returns incomplete JSON (missing `inputTokenLimit` etc.) | Low |
| `modules/account/` + `api/quota.rs` | JSON and PostgreSQL have different UUIDs for same accounts (dual-storage ID mismatch). `repo.update_quota(json_id)` fails with FK violation because JSON IDs don't exist in PostgreSQL | High |
| `api/quota.rs` `toggle_proxy_status` | Uses repo-only write (not dual-write) when repo exists â€” if repo goes down, JSON has stale `proxy_disabled` field | Medium |
| `proxy/middleware/monitor.rs` | `std::str::from_utf8(&chunk)` on raw stream chunks â€” multi-byte UTF-8 split across chunk boundaries causes decoding failure, chunk skipped in line_buffer | Medium |
| `proxy/middleware/monitor.rs` | SSE processing loop ignores `tx.send()` errors â€” if client disconnects, middleware continues consuming entire upstream stream (wasted resources) | Medium |
| `proxy/middleware/monitor.rs` | `line_buffer = line_buffer[newline_pos + 1..].to_string()` allocates new String for every SSE line â€” excessive allocation churn | Low |

---

## ğŸ§  SMART ROUTING ARCHITECTURE [2026-01-30]

**Replaces:** Old 3-mode system (CacheFirst/Balance/PerformanceFirst)

### Problem Solved

Thundering herd + cache destruction pattern:
```
10 concurrent requests â†’ Account A
   â†“
Account A: 429 (rate limit)
   â†“
ALL 10 requests switch to Account B
   â†“
Account B: instant 429 (thundering herd)
   â†“
Cache on A â€” lost, Cache on B â€” never built
   â†“
cache_hit â‰ˆ 0%
```

### Solution: Unified Smart Routing

| Component | Description |
|-----------|-------------|
| **Least-Connections Selection** | Route to account with fewest active requests (not round-robin) |
| **Per-Account Concurrency Limit** | Max N concurrent requests per account (default: 3) |
| **Isolated Session Migration** | On 429: migrate THIS request only, keep session binding intact |
| **AIMD Pre-emptive Check** | Skip accounts with usage_ratio > 1.2 |

### Configuration (`SmartRoutingConfig`)

```rust
pub struct SmartRoutingConfig {
    pub max_concurrent_per_account: u32,  // default: 3
    pub preemptive_throttle_ratio: f32,   // default: 0.8
    pub throttle_delay_ms: u64,           // default: 100
    pub enable_session_affinity: bool,    // default: true
}
```

### Key Behavioral Changes

| Before | After |
|--------|-------|
| 429 â†’ unbind session â†’ ALL requests migrate | 429 â†’ keep binding â†’ only THIS request migrates |
| QUOTA_EXHAUSTED: 5min lockout | QUOTA_EXHAUSTED: 10min fallback + dynamic protected_models |
| Session stuck on exhausted account | Unbind after 3 consecutive failures OR lockout > 5min |
| Round-robin account selection | Least-connections (min active requests) |
| 3 manual modes to choose | 1 unified algorithm with tunable params |
| No concurrency limit | Max N per account prevents thundering herd |

### Tier-Priority Account Selection [2026-02-02]

**Selection Order:**
| Priority | Tier | Example |
|----------|------|---------|
| 0 | ultra-business | `ws-ai-ultra-business-tier` |
| 1 | ultra | `g1-ultra-tier` |
| 2 | pro | `g1-pro-tier` |
| 3 | free | `free-tier` |
| 4 | unknown | (no tier info) |

**Algorithm (updated 2026-02-02):**
1. **Ultra-tier priority:** Check ultra/ultra-business accounts (tier 0-1) FIRST
2. If ultra available â†’ use it (even if session is sticky to pro account)
3. If no ultra available â†’ check sticky session binding (fallback)
4. If no sticky â†’ standard tier-priority selection from remaining accounts
5. Filter at each step: exclude rate-limited, quota-protected, already-attempted
6. Sort by: tier_priority (ascending), then active_requests (ascending)

**Behavior:**
- Ultra accounts OVERRIDE sticky sessions â€” if ultra is available, it's used
- Sticky session acts as FALLBACK when no ultra accounts are available
- All accounts respect rate limits with short lockout (5s)
- Ultra accounts share load via least-connections within tier

**Example Flow:**
| Scenario | Sticky on Pro | Ultra Available | Result |
|----------|---------------|-----------------|--------|
| Normal | Account A (pro) | Account B (ultra) free | B selected (ultra priority) |
| Ultra busy | Account A (pro) | B rate-limited | A selected (sticky fallback) |
| Both busy | Account A (pro) | B rate-limited, A rate-limited | Next available by tier |

---

## ğŸ›¡ï¸ FINGERPRINT PROTECTION [2026-02-01]

### Device Fingerprint API

**Status:** IMPLEMENTED

REST API for managing Cursor/VSCode device fingerprints (storage.json):

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/device/profile` | GET | Read current device profile from storage.json |
| `/api/device/profile` | POST | Generate new profile and write to storage.json |
| `/api/device/backup` | POST | Create timestamped backup of storage.json |
| `/api/device/baseline` | GET | Get original baseline profile |

### Known Limitations

| Protection | Status | Notes |
|------------|--------|-------|
| Device fingerprint API | âœ… IMPLEMENTED | REST endpoints for profile management |
| User-Agent rotation | âŒ REVERTED | Caused CONSUMER_INVALID errors from Google |
| WARP IP isolation | âŒ REMOVED | Module deleted [2026-02-08] â€” Google detects WARP â†’ stricter rate limits |
| TLS/JA3 fingerprint | âŒ MISSING | Would require custom TLS config |
| HTTP header randomization | âŒ MISSING | Accept-Language, etc. |

---

## ğŸ”§ API Endpoints

```bash
# Health status (account availability)
GET /api/resilience/health

# Circuit breaker states
GET /api/resilience/circuits

# AIMD rate limiting stats
GET /api/resilience/aimd

# Prometheus metrics
GET /api/metrics
```

---

## âœ… Verification Commands

```bash
cargo check --workspace                        # âœ… passes
cargo clippy --workspace -- -Dwarnings         # âœ… passes
cargo test -p antigravity-types                # âœ… 7 tests pass
```

### Load Testing (MANDATORY for routing/selection changes)

Changes to account selection, routing, or rate limiting logic MUST be verified with load testing before production deployment.

**Test command (50 concurrent requests):**
```bash
for i in $(seq 1 50); do
  curl -s -X POST "https://antigravity.quantumind.ru/v1/chat/completions" \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"model\": \"gemini-3-pro\", \"messages\": [{\"role\": \"user\", \"content\": \"Say $i. ID: $(uuidgen)\"}], \"max_tokens\": 50}" &
done
wait
```

**Success criteria:**
- 100% success rate (all HTTP 200)
- No thundering herd (requests distributed across multiple accounts)
- Total time < 30s for 50 requests

**When required:**
- Any change to `token_manager/mod.rs`
- Any change to `rate_limit/` modules
- Any change to account selection/routing logic

```bash
cargo test -p antigravity-core --lib           # âœ… 170 tests pass
cargo build --release -p antigravity-server    # âœ… builds (1m 22s, 11MB)
```

---

## ğŸ”€ Upstream Sync

- **repo:** lbjlaq/Antigravity-Manager
- **watch:** src-tauri/src/proxy/, src-tauri/src/modules/
- **ignore:** *.tsx, *.json, README*, i18n/, Tauri-specific
- **last_reviewed:** f86a58d (2026-02-08)

### What We Port

âœ… Bug fixes in protocol transformation, new model support, JSON Schema improvements, security fixes

âŒ UI/React, Tauri-specific, changes conflicting with our resilience layer

### Our Divergences

| Area | Description |
|------|-------------|
| Routing | Smart routing with least-connections (not P2C/round-robin) |
| Resilience | AIMD rate limiting, circuit breakers, health scores |
| Handlers | Axum-specific, streaming SSE, buffer overflow protection |
| Security | Constant-time API key comparison |

---

## âš ï¸ KNOWN ARCHITECTURAL QUIRK: Shared Project Rate Limits [2026-01-18]

Rate limits are tracked per **account_id**, but Google Cloud quotas are enforced per **project_id**. If two accounts share the same project, switching between them won't help â€” both will hit 429.

**Why We DON'T Fix This (Yet):** Google's prompt caching is tied to `project_id`. Switching to another account in the same project might still benefit from cached prompts.

```bash
# Check for shared projects:
cat ~/.antigravity_tools/accounts/*.json | jq -r '.token.project_id' | sort | uniq -c
```

---

## ğŸ” BACKEND DISCOVERY: Model Routing [2026-01-18]

| Model Alias | Actual Backend | Evidence |
|-------------|----------------|----------|
| `gpt-4o`, `gpt-4o-mini`, `gpt-*` | **Gemini** (alias) | Responds: "I am gemini-1.5-flash-pro" |
| `gemini-3-pro`, `gemini-*` | **Gemini** (native) | Responds with Antigravity system prompt |
| `claude-opus-4-5`, `claude-*` | **Claude via Vertex AI** | Error contains `req_vrtx_*` request ID |

**Key Insights:** GPT models are fake (Gemini with OpenAI format). Claude models are REAL (Vertex AI partnership).

---

## âš ï¸ UNDOCUMENTED OUTPUT TOKEN LIMIT [2026-01-19]

### The Problem

Google Antigravity API has an **undocumented output limit of ~4K tokens** (~150-200 lines of code).

**Symptoms:**
- Stream cuts mid-response without `finish_reason: "max_tokens"`
- Tool call JSON left incomplete/invalid
- Client receives garbage, cannot parse response
- No error message â€” just silent truncation

**Empirical evidence:** Max observed output in 24h of logs = 3901 tokens.

### What This Means

| Operation | Risk |
|-----------|------|
| Edit tool (small diffs) | âœ… Safe |
| Write tool (<100 lines) | âœ… Safe |
| Write tool (>150 lines) | âŒ Will be truncated |
| README generation | âŒ High risk |
| Full file creation | âŒ High risk |

### Workaround

For large files, use incremental approach:
1. Write skeleton with TODO markers
2. Fill each section with separate Edit calls
3. Each operation <100 lines

**Status:** No fix implemented. Using system prompt workaround (see global AGENTS.md rule 20).

---

## âš ï¸ MANDATORY thoughtSignature ON functionCall PARTS [2026-02-08]

Google now **requires** `thoughtSignature` field on ALL `functionCall` parts in request body. Without it â†’ 400 INVALID_ARGUMENT.

**Error message:** `"Function call is missing a thought_signature in functionCall parts. This is required for tools to work correctly."`

**Docs:** https://ai.google.dev/gemini-api/docs/thought-signatures

**Fix applied:** Both OpenAI path (`message_transform.rs`) and Gemini native path (`wrapper.rs`) now inject `"skip_thought_signature_validator"` dummy signature when no real signature is cached. This dummy is documented by Google as a valid bypass.

**Injection points:**
| Path | File | Behavior |
|------|------|----------|
| OpenAI | `mappers/openai/request/message_transform.rs:154` | Session cache â†’ fallback to dummy |
| Gemini native | `mappers/gemini/wrapper.rs:23` | Session cache â†’ fallback to dummy |
| Claude/Anthropic | `mappers/claude/request/content_builder.rs:253` | Already had dummy fallback (unaffected) |

**Scope:** Applies to ALL Gemini models. Claude via Vertex AI is unaffected (different protocol).

---

## âš ï¸ INPUT TOKEN LIMIT â€” CLAUDE VIA VERTEX AI [2026-02-07]

Vertex AI enforces a **hard 200,000 token limit** on Claude prompt input.

| Test | Prompt Tokens | Result | Time |
|------|---------------|--------|------|
| ~200K chars | 170,841 | âœ… 200 OK | 6.4s |
| ~1.2M chars | 278,399 | âŒ 400 `prompt is too long: 278399 tokens > 200000 maximum` | 1.6s |

**Error format:** `{"type":"error","error":{"type":"invalid_request_error","message":"prompt is too long: N tokens > 200000 maximum"}}` with `request_id: req_vrtx_*`

**Source of limit:** Google Vertex AI side (confirmed by `req_vrtx_*` request ID), not Antigravity proxy.

**Note:** Gemini models have separate limits (1M+ context window). This 200K limit applies ONLY to Claude models routed through Vertex AI.

---

## ğŸš€ DEPLOYMENT [2026-02-07]

### Canonical Path: Nix Closure

**Single entry point:** `./deploy.sh <command>`

| Command | Description |
|---------|-------------|
| `./deploy.sh deploy` | Build Nix closure â†’ copy to VPS â†’ restart service |
| `./deploy.sh rollback` | Restore previous version from `.previous` backup |
| `./deploy.sh status` | Show service status, health, current/previous binary |
| `./deploy.sh deploy-local` | Zero-downtime local deploy (socket activation) |
| `./deploy.sh logs [-n N]` | Stream VPS service logs |

**Flow:**
```
Local: nix build .#antigravity-server
  â†’ nix copy --to ssh://vps-production (closure with all deps)
  â†’ rsync frontend dist/
  â†’ SSH: symlink binary, write systemd unit, restart
  â†’ Health check: /api/health
```

**Rollback:** Each deploy saves previous binary path to `/opt/antigravity/.previous`. Rollback restores symlink + restarts.

### Zero-Downtime (Local) â€” Socket Activation [2026-02-08]

Local service uses **systemd socket activation** for zero-downtime deploys:

```
antigravity-manager.socket â† owns port 8045 (always listening)
         â†“ (first connection)
antigravity-manager.service â† started by systemd, receives fd=3
         â†“ (deploy: systemctl restart)
[OLD draining] â† socket stays open, kernel buffers new connections (backlog=4096)
         â†“ (OLD exits, NEW starts, receives fd=3)
[NEW] â† processes buffered connections (~3s latency during restart)
```

**Command:** `./deploy.sh deploy-local` (build â†’ replace binary â†’ restart service)

**Units:**
- `~/.config/systemd/user/antigravity-manager.socket` â€” ListenStream=127.0.0.1:8045
- `~/.config/systemd/user/antigravity-manager.service` â€” Requires=antigravity-manager.socket

### Important: Unified Build

**Backend and frontend are built together** via `build.rs`. `cargo build -p antigravity-server` builds BOTH Rust backend and Leptos WASM frontend (via trunk).

**DO NOT deploy backend without rebuilding frontend** â€” they share the same release cycle.

---

## ğŸ“¦ BUILD SYSTEM [2026-01-19]

### Build Commands

| Command | What it builds |
|---------|---------------|
| `cargo build -p antigravity-server` | Backend + Frontend (via build.rs) |
| `trunk build` (in src-leptos/) | Frontend only |
| `cargo build -p antigravity-leptos` | Frontend crate only (no WASM) |

### Environment Variables

| Variable | Default | Purpose |
|----------|---------|---------|
| `ANTIGRAVITY_STATIC_DIR` | `./src-leptos/dist` | Path to frontend assets |
| `ANTIGRAVITY_PORT` | `8045` | Server port |
| `SKIP_TRUNK_BUILD` | unset | Skip frontend build in CI |
| `ANTIGRAVITY_SYNC_REMOTE` | unset | Remote server URL for bidirectional config sync |

---

## ğŸ”„ MODEL ROUTING SYNC [2026-01-27]

### Overview

Bidirectional synchronization of `custom_mapping` (model routing) between VPS and local instances using **Last-Write-Wins (LWW)** merge strategy â€” the same approach used by AWS DynamoDB and Apache Cassandra for distributed state.

### How It Works

```
VPS (antigravity.quantumind.ru)          Local Machine
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SyncableMapping (MASTER)   â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  SyncableMapping        â”‚
â”‚                             â”‚         â”‚                         â”‚
â”‚  GET /api/config/mapping    â”‚         â”‚  Auto-sync every 60s    â”‚
â”‚  POST /api/config/mapping   â”‚         â”‚  (if ANTIGRAVITY_SYNC_  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   REMOTE is set)        â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LWW Merge Strategy

Each mapping entry has a timestamp. On merge:
- If only in local â†’ keep local
- If only in remote â†’ add from remote  
- If in both â†’ keep the one with **higher timestamp**
- On timestamp tie â†’ lexicographic comparison of `target` as tie-breaker

No conflicts. Eventual consistency guaranteed.

### Tombstone Support

Deletions use **tombstones** (soft delete) to prevent "zombie resurrection" during sync:
- `remove(key)` â†’ inserts tombstone entry with `deleted: true`
- Tombstones propagate via LWW like regular entries
- `get()`, `len()`, `to_simple_map()` exclude tombstones
- `total_entries()` includes tombstones (for debugging)

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/config/mapping` | GET | Get current mappings with timestamps |
| `/api/config/mapping` | POST | Merge remote mappings (LWW) |

### Usage

**Enable sync on local machine:**
```bash
export ANTIGRAVITY_SYNC_REMOTE="https://antigravity.quantumind.ru"
antigravity-server
```

**Manual sync via API:**
```bash
# Get current mappings from VPS
curl https://antigravity.quantumind.ru/api/config/mapping

# Push local changes to VPS
curl -X POST https://antigravity.quantumind.ru/api/config/mapping \
  -H "Content-Type: application/json" \
  -d '{"mapping": {"entries": {"gpt-4o": {"target": "gemini-3-pro", "updated_at": 1737932400000}}}}'
```

### Sync Flow

1. Every 60s, local fetches remote mappings
2. LWW merge: newer entries overwrite older
3. Local sends back any entries that are newer locally
4. Both instances converge to identical state

### Known Limitations

- **Tombstone persistence:** Currently tombstones are not fully persisted. Deletions set `target=""` which effectively disables the mapping, but the key remains in storage. Full tombstone garbage collection is TODO.
- **Storage format:** Runtime uses `SyncableMapping` with timestamps, but persistence uses legacy `HashMap<String, String>` format in `gui_config.json`. Timestamps are stored separately in memory.

### Files

- `crates/antigravity-types/src/models/sync.rs` â€” `SyncableMapping`, `MappingEntry`, LWW merge logic
- `antigravity-server/src/config_sync.rs` â€” Auto-sync background task
- `antigravity-server/src/state.rs` â€” `get_syncable_mapping()`, `merge_remote_mapping()`
- `antigravity-server/src/api.rs` â€” `/api/config/mapping` endpoints
